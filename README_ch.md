<div align="center">

<h1> OpenOCR: A general OCR system with accuracy and efficiency </h1>

<h5 align="center"> å¦‚æœæ‚¨è§‰å¾—æœ¬é¡¹ç›®æœ‰å¸®åŠ©ï¼Œè¯·ä¸ºæˆ‘ä»¬ç‚¹äº®StarğŸŒŸ </h5>

<a href="https://github.com/Topdu/OpenOCR/blob/main/LICENSE"><img alt="license" src="https://img.shields.io/github/license/Topdu/OpenOCR"></a>
<a href='https://arxiv.org/abs/2411.15858'><img src='https://img.shields.io/badge/è®ºæ–‡-Arxiv-red'></a>
<a href="https://huggingface.co/spaces/topdu/OpenOCR-Demo" target="_blank"><img src="https://img.shields.io/badge/%F0%9F%A4%97-Hugging Face Demo-blue"></a>
<a href="https://modelscope.cn/studios/topdktu/OpenOCR-Demo" target="_blank"><img src="https://img.shields.io/badge/é­”æ­-Demo-blue"></a>
<a href=""><img src="https://img.shields.io/badge/OS-Linux%2C%20Win%2C%20Mac-pink.svg"></a>
<a href="https://github.com/Topdu/OpenOCR/graphs/contributors"><img src="https://img.shields.io/github/contributors/Topdu/OpenOCR?color=9ea"></a>
<a href="https://pepy.tech/project/openocr"><img src="https://static.pepy.tech/personalized-badge/openocr?period=total&units=abbreviation&left_color=grey&right_color=blue&left_text=Clone%20ä¸‹è½½é‡"></a>
<a href="https://github.com/Topdu/OpenOCR/stargazers"><img src="https://img.shields.io/github/stars/Topdu/OpenOCR?color=ccf"></a>
<a href="https://pypi.org/project/openocr-python/"><img alt="PyPI" src="https://img.shields.io/pypi/v/openocr-python"><img src="https://img.shields.io/pypi/dm/openocr-python?label=PyPI%20ä¸‹è½½é‡"></a>

<a href="#å¿«é€Ÿå¼€å§‹"> ğŸš€ å¿«é€Ÿå¼€å§‹ </a> | ç®€ä½“ä¸­æ–‡ | [English](./README.md)

</div>

______________________________________________________________________

æˆ‘ä»¬è‡´åŠ›äºæ„å»ºåœºæ™¯æ–‡æœ¬æ£€æµ‹ä¸è¯†åˆ«æ¨¡å‹çš„ç»Ÿä¸€è®­ç»ƒè¯„ä¼°åŸºå‡†ã€‚åŸºäºæ­¤åŸºå‡†ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å…¼é¡¾ç²¾åº¦ä¸æ•ˆç‡çš„é€šç”¨OCRç³»ç»Ÿâ€”â€”**OpenOCR**ã€‚æœ¬ä»“åº“åŒæ—¶ä½œä¸ºå¤æ—¦å¤§å­¦[FVLå®éªŒå®¤](https://fvl.fudan.edu.cn)OCRå›¢é˜Ÿçš„å®˜æ–¹ä»£ç åº“ã€‚

æˆ‘ä»¬è¯šæŒšæ¬¢è¿ç ”ç©¶è€…æ¨èOCRç›¸å…³ç®—æ³•ï¼Œå¹¶æŒ‡å‡ºæ½œåœ¨çš„äº‹å®æ€§é”™è¯¯æˆ–ä»£ç ç¼ºé™·ã€‚æ”¶åˆ°å»ºè®®åï¼Œæˆ‘ä»¬å°†åŠæ—¶è¯„ä¼°å¹¶ä¸¥è°¨å¤ç°ã€‚æœŸå¾…ä¸æ‚¨æºæ‰‹æ¨è¿›OpenOCRå‘å±•ï¼ŒæŒç»­ä¸ºOCRç¤¾åŒºè´¡çŒ®åŠ›é‡ï¼

## æ ¸å¿ƒç‰¹æ€§

- ğŸ”¥**OpenOCR: A general OCR system with accuracy and efficiency**

  - âš¡\[[å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)\] \[[æ¨¡å‹ä¸‹è½½](https://github.com/Topdu/OpenOCR/releases/tag/develop0.0.1)\] \[[ModelScope Demo](https://modelscope.cn/studios/topdktu/OpenOCR-Demo)\] \[[Hugging Face Demo](https://huggingface.co/spaces/topdu/OpenOCR-Demo)\] \[[æœ¬åœ°Demo](#æœ¬åœ°Demo)\] \[[PaddleOCRå®ç°](https://paddlepaddle.github.io/PaddleOCR/latest/algorithm/text_recognition/algorithm_rec_svtrv2.html)\]
  - [æŠ€æœ¯æ–‡æ¡£](./docs/openocr.md)
    - åŸºäºSVTRv2æ„å»ºçš„å®ç”¨OCRç³»ç»Ÿ
    - åœ¨[OCRç«èµ›æ¦œå•](https://aistudio.baidu.com/competition/detail/1131/0/leaderboard)ä¸Šï¼Œç²¾åº¦è¶…è¶Š[PP-OCRv4](https://paddlepaddle.github.io/PaddleOCR/latest/ppocr/model_list.html)åŸºçº¿4.5%ï¼Œæ¨ç†é€Ÿåº¦ä¿æŒç›¸è¿‘
    - [x] æ”¯æŒä¸­è‹±æ–‡æ–‡æœ¬æ£€æµ‹ä¸è¯†åˆ«
    - [x] æä¾›æœåŠ¡å™¨ç«¯(Server)ä¸ç§»åŠ¨ç«¯(mobile)æ¨¡å‹
    - [x] æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†å¾®è°ƒ: [æ£€æµ‹æ¨¡å‹å¾®è°ƒ](./docs/finetune_det.md), [è¯†åˆ«æ¨¡å‹å¾®è°ƒ](./docs/finetune_rec.md)
    - [x] [æ”¯æŒå¯¼å‡ºONNXæ¨¡å‹](#å¯¼å‡ºonnxæ¨¡å‹)

- ğŸ”¥**SVTRv2: CTC Beats Encoder-Decoder Models in Scene Text Recognition**

  - \[[è®ºæ–‡](https://arxiv.org/abs/2411.15858)\] \[[æ–‡æ¡£](./configs/rec/svtrv2/)\] \[[æ¨¡å‹](./configs/rec/svtrv2/readme.md#11-models-and-results)\] \[[æ•°æ®é›†](./docs/svtrv2.md#downloading-datasets)\] \[[é…ç½®/è®­ç»ƒ/æ¨ç†](./configs/rec/svtrv2/readme.md#3-model-training--evaluation)\] \[[åŸºå‡†æµ‹è¯•](./docs/svtrv2.md#results-benchmark--configs--checkpoints)\]
  - [æŠ€æœ¯æ–‡æ¡£](./docs/svtrv2.md)
    - åŸºäº[Union14M](https://github.com/Mountchicken/Union14M)æ„å»ºçš„åœºæ™¯æ–‡æœ¬è¯†åˆ«ç»Ÿä¸€è®­ç»ƒè¯„ä¼°åŸºå‡†
    - æ”¯æŒ24ç§åœºæ™¯æ–‡æœ¬è¯†åˆ«æ–¹æ³•åœ¨å¤§è§„æ¨¡çœŸå®æ•°æ®é›†[Union14M-L-Filter](./docs/svtrv2.md#æ•°æ®é›†è¯¦æƒ…)ä¸Šçš„è®­ç»ƒï¼Œå°†æŒç»­é›†æˆå‰æ²¿æ–¹æ³•
    - ç›¸æ¯”åŸºäºåˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œç²¾åº¦æå‡20-30%
    - å•ä¸€è§†è§‰æ¨¡å‹å®ç°ä»»æ„å½¢çŠ¶æ–‡æœ¬è¯†åˆ«ä¸è¯­è¨€å»ºæ¨¡
    - åœ¨ç²¾åº¦ä¸é€Ÿåº¦ä¸Šå…¨é¢è¶…è¶ŠåŸºäºAttentionçš„ç¼–è§£ç æ¨¡å‹
    - [ä»é›¶è®­ç»ƒSOTAæ¨¡å‹æŒ‡å—](./docs/svtrv2.md#get-started-with-training-a-sota-scene-text-recognition-model-from-scratch)

## è‡ªç ”STRç®—æ³•

- [**SVTRv2**](./configs/rec/svtrv2) (*Yongkun Du, Zhineng Chen\*, Hongtao Xie, Caiyan Jia, Yu-Gang Jiang. SVTRv2: CTC Beats Encoder-Decoder Models in Scene Text Recognition,* ICCV 2025. [Doc](./configs/rec/svtrv2/), [Paper](https://arxiv.org/abs/2411.15858))
- [**IGTR**](./configs/rec/igtr/) (*Yongkun Du, Zhineng Chen\*, Yuchen Su, Caiyan Jia, Yu-Gang Jiang. Instruction-Guided Scene Text Recognition,* TPAMI 2025. [Doc](./configs/rec/igtr), [Paper](https://ieeexplore.ieee.org/document/10820836))
- [**CPPD**](./configs/rec/cppd/) (*Yongkun Du, Zhineng Chen\*, Caiyan Jia, Xiaoting Yin, Chenxia Li, Yuning Du, Yu-Gang Jiang. Context Perception Parallel Decoder for Scene Text Recognition,* TPAMI 2025. [PaddleOCR Doc](https://github.com/PaddlePaddle/PaddleOCR/blob/main/docs/algorithm/text_recognition/algorithm_rec_cppd.en.md), [Paper](https://ieeexplore.ieee.org/document/10902187))
- [**SMTR&FocalSVTR**](./configs/rec/smtr/) (*Yongkun Du, Zhineng Chen\*, Caiyan Jia, Xieping Gao, Yu-Gang Jiang. Out of Length Text Recognition with Sub-String Matching,* AAAI 2025. [Doc](./configs/rec/smtr/), [Paper](https://arxiv.org/abs/2407.12317))
- [**DPTR**](./configs/rec/dptr/) (*Shuai Zhao, Yongkun Du, Zhineng Chen\*, Yu-Gang Jiang. Decoder Pre-Training with only Text for Scene Text Recognition,* ACM MM 2024. [Paper](https://arxiv.org/abs/2408.05706))
- [**CDistNet**](./configs/rec/cdistnet/) (*Tianlun Zheng, Zhineng Chen\*, Shancheng Fang, Hongtao Xie, Yu-Gang Jiang. CDistNet: Perceiving Multi-Domain Character Distance for Robust Text Recognition,* IJCV 2024. [Paper](https://link.springer.com/article/10.1007/s11263-023-01880-0))
- **MRN** (*Tianlun Zheng, Zhineng Chen\*, Bingchen Huang, Wei Zhang, Yu-Gang Jiang. MRN: Multiplexed Routing Network for Incremental Multilingual Text Recognition,* ICCV 2023. [Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_MRN_Multiplexed_Routing_Network_for_Incremental_Multilingual_Text_Recognition_ICCV_2023_paper.html), [Code](https://github.com/simplify23/MRN))
- **TPS++** (*Tianlun Zheng, Zhineng Chen\*, Jinfeng Bai, Hongtao Xie, Yu-Gang Jiang. TPS++: Attention-Enhanced Thin-Plate Spline for Scene Text Recognition,* IJCAI 2023. [Paper](https://arxiv.org/abs/2305.05322), [Code](https://github.com/simplify23/TPS_PP))
- [**SVTR**](./configs/rec/svtr/) (*Yongkun Du, Zhineng Chen\*, Caiyan Jia, Xiaoting Yin, Tianlun Zheng, Chenxia Li, Yuning Du, Yu-Gang Jiang. SVTR: Scene Text Recognition with a Single Visual Model,* IJCAI 2022 (Long). [PaddleOCR Doc](https://github.com/Topdu/PaddleOCR/blob/main/doc/doc_ch/algorithm_rec_svtr.md), [Paper](https://www.ijcai.org/proceedings/2022/124))
- [**NRTR**](./configs/rec/nrtr/) (*Fenfen Sheng, Zhineng Chen, Bo Xu. NRTR: A No-Recurrence Sequence-to-Sequence Model For Scene Text Recognition,* ICDAR 2019. [Paper](https://arxiv.org/abs/1806.00926))

## è¿‘æœŸæ›´æ–°

- **2025.07.10**: [SVTRv2](https://arxiv.org/abs/2411.15858)è¢«ICCV 2025æ¥æ”¶. è¯¦è§[æ–‡æ¡£](./configs/rec/svtrv2/)
- **2025.03.24**: ğŸ”¥ å‘å¸ƒè‡ªå®šä¹‰æ•°æ®é›†å¾®è°ƒåŠŸèƒ½: [æ£€æµ‹æ¨¡å‹å¾®è°ƒ](./docs/finetune_det.md), [è¯†åˆ«æ¨¡å‹å¾®è°ƒ](./docs/finetune_rec.md)
- **2025.03.23**: ğŸ”¥ æ–°å¢[ONNXæ¨¡å‹å¯¼å‡ºåŠŸèƒ½](#å¯¼å‡ºonnxæ¨¡å‹)
- **2025.02.22**: [CPPD](https://ieeexplore.ieee.org/document/10902187)è®ºæ–‡è¢«TPAMIå½•ç”¨ï¼Œè¯¦è§[æ–‡æ¡£](./configs/rec/cppd/)ä¸[PaddleOCRæ–‡æ¡£](https://github.com/PaddlePaddle/PaddleOCR/blob/main/docs/algorithm/text_recognition/algorithm_rec_cppd.en.md)
- **2024.12.31**: [IGTR](https://ieeexplore.ieee.org/document/10820836)è®ºæ–‡è¢«TPAMIå½•ç”¨ï¼Œè¯¦è§[æ–‡æ¡£](./configs/rec/igtr/)
- **2024.12.16**: [SMTR](https://arxiv.org/abs/2407.12317)è®ºæ–‡è¢«AAAI 2025å½•ç”¨ï¼Œè¯¦è§[æ–‡æ¡£](./configs/rec/smtr/)
- **2024.12.03**: [DPTR](https://arxiv.org/abs/2408.05706)é¢„è®­ç»ƒä»£ç åˆå¹¶
- **ğŸ”¥ 2024.11.23 é‡å¤§æ›´æ–°**:
  - **OpenOCRé€šç”¨OCRç³»ç»Ÿå‘å¸ƒ**
    - âš¡\[[å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)\] \[[æ¨¡å‹ä¸‹è½½](https://github.com/Topdu/OpenOCR/releases/tag/develop0.0.1)\] \[[ModelScopeDemo](https://modelscope.cn/studios/topdktu/OpenOCR-Demo)\] \[[Hugging FaceDemo](https://huggingface.co/spaces/topdu/OpenOCR-Demo)\] \[[æœ¬åœ°Demo](#æœ¬åœ°Demo)\] \[[PaddleOCRå®ç°](https://paddlepaddle.github.io/PaddleOCR/latest/algorithm/text_recognition/algorithm_rec_svtrv2.html)\]
    - [æŠ€æœ¯æ–‡æ¡£](./docs/openocr.md)
  - **SVTRv2è®ºæ–‡å‘å¸ƒ**
    - \[[è®ºæ–‡](https://arxiv.org/abs/2411.15858)\] \[[æ–‡æ¡£](./configs/rec/svtrv2/)\] \[[æ¨¡å‹](./configs/rec/svtrv2/readme.md#11-models-and-results)\] \[[æ•°æ®é›†](./docs/svtrv2.md#downloading-datasets)\] \[[é…ç½®/è®­ç»ƒ/æ¨ç†](./configs/rec/svtrv2/readme.md#3-model-training--evaluation)\] \[[åŸºå‡†æµ‹è¯•](./docs/svtrv2.md#results-benchmark--configs--checkpoints)\]
    - [æŠ€æœ¯æ–‡æ¡£](./docs/svtrv2.md)
    - [ä»é›¶è®­ç»ƒSOTAæ¨¡å‹æŒ‡å—](./docs/svtrv2.md#get-started-with-training-a-sota-scene-text-recognition-model-from-scratch)

## å¿«é€Ÿå¼€å§‹

**æ³¨æ„**: OpenOCRæ”¯æŒONNXå’ŒPyTorchåŒæ¡†æ¶æ¨ç†ï¼Œç¯å¢ƒç›¸äº’ç‹¬ç«‹ã€‚ä½¿ç”¨ONNXæ¨ç†æ—¶æ— éœ€å®‰è£…PyTorchï¼Œåä¹‹äº¦ç„¶ã€‚

### 1. ONNXæ¨ç†

#### å®‰è£…OpenOCRåŠä¾èµ–:

```shell
pip install openocr-python
pip install onnxruntime
```

#### ä½¿ç”¨ç¤ºä¾‹:

```python
from openocr import OpenOCR
onnx_engine = OpenOCR(backend='onnx', device='cpu')
img_path = '/path/img_path or /path/img_file'
result, elapse = onnx_engine(img_path)
```

### 2. PyTorchæ¨ç†

#### ç¯å¢ƒä¾èµ–:

- [PyTorch](http://pytorch.org/) >= 1.13.0
- Python >= 3.7

```shell
conda create -n openocr python==3.8
conda activate openocr
# å®‰è£…GPUç‰ˆæœ¬
conda install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 pytorch-cuda=11.8 -c pytorch -c nvidia
# æˆ–CPUç‰ˆæœ¬
conda install pytorch torchvision torchaudio cpuonly -c pytorch
```

#### 2.1 PythonåŒ…å®‰è£…

**å®‰è£…OpenOCR**:

```shell
pip install openocr-python
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from openocr import OpenOCR
engine = OpenOCR()
img_path = '/path/img_path or /path/img_file'
result, elapse = engine(img_path)

# Serveræ¨¡å¼
# engine = OpenOCR(mode='server')
```

#### 2.2 æºç å®‰è£…

```shell
git clone https://github.com/Topdu/OpenOCR.git
cd OpenOCR
pip install -r requirements.txt
wget https://github.com/Topdu/OpenOCR/releases/download/develop0.0.1/openocr_det_repvit_ch.pth
wget https://github.com/Topdu/OpenOCR/releases/download/develop0.0.1/openocr_repsvtr_ch.pth
# Serverè¯†åˆ«æ¨¡å‹
# wget https://github.com/Topdu/OpenOCR/releases/download/develop0.0.1/openocr_svtrv2_ch.pth
```

**ä½¿ç”¨å‘½ä»¤**:

```shell
# ç«¯åˆ°ç«¯OCRç³»ç»Ÿ: æ£€æµ‹+è¯†åˆ«
python tools/infer_e2e.py --img_path=/path/img_path or /path/img_file
# å•ç‹¬æ£€æµ‹æ¨¡å‹
python tools/infer_det.py --c ./configs/det/dbnet/repvit_db.yml --o Global.infer_img=/path/img_path or /path/img_file
# å•ç‹¬è¯†åˆ«æ¨¡å‹
python tools/infer_rec.py --c ./configs/rec/svtrv2/repsvtr_ch.yml --o Global.infer_img=/path/img_path or /path/img_file
```

##### å¯¼å‡ºONNXæ¨¡å‹

```shell
pip install onnx
python tools/toonnx.py --c configs/rec/svtrv2/repsvtr_ch.yml --o Global.device=cpu
python tools/toonnx.py --c configs/det/dbnet/repvit_db.yml --o Global.device=cpu
```

##### ONNXRuntimeæ¨ç†

```shell
pip install onnxruntime
# ç«¯åˆ°ç«¯OCRç³»ç»Ÿ
python tools/infer_e2e.py --img_path=/path/img_path or /path/img_file --backend=onnx --device=cpu
# æ£€æµ‹æ¨¡å‹
python tools/infer_det.py --c ./configs/det/dbnet/repvit_db.yml --o Global.backend=onnx Global.device=cpu Global.infer_img=/path/img_path or /path/img_file
# è¯†åˆ«æ¨¡å‹
python tools/infer_rec.py --c ./configs/rec/svtrv2/repsvtr_ch.yml --o Global.backend=onnx Global.device=cpu Global.infer_img=/path/img_path or /path/img_file
```

#### æœ¬åœ°Demo

```shell
pip install gradio==4.20.0
wget https://github.com/Topdu/OpenOCR/releases/download/develop0.0.1/OCR_e2e_img.tar
tar xf OCR_e2e_img.tar
# å¯åŠ¨Demo
python demo_gradio.py
```

## ç®—æ³•å¤ç°è®¡åˆ’

### åœºæ™¯æ–‡æœ¬è¯†åˆ«(STR)

| æ–¹æ³•                                          | ä¼šè®®/æœŸåˆŠ                                                                                        | è®­ç»ƒæ”¯æŒ | è¯„ä¼°æ”¯æŒ | è´¡çŒ®è€…                                      |
| --------------------------------------------- | ------------------------------------------------------------------------------------------------ | -------- | -------- | ------------------------------------------- |
| [CRNN](./configs/rec/svtrs/)                  | [TPAMI 2016](https://arxiv.org/abs/1507.05717)                                                   | âœ…       | âœ…       |                                             |
| [ASTER](./configs/rec/aster/)                 | [TPAMI 2019](https://ieeexplore.ieee.org/document/8395027)                                       | âœ…       | âœ…       | [pretto0](https://github.com/pretto0)       |
| [NRTR](./configs/rec/nrtr/)                   | [ICDAR 2019](https://arxiv.org/abs/1806.00926)                                                   | âœ…       | âœ…       |                                             |
| [SAR](./configs/rec/sar/)                     | [AAAI 2019](https://aaai.org/papers/08610-show-attend-and-read-a-simple-and-strong-baseline-for-irregular-text-recognition/) | âœ…       | âœ…       | [pretto0](https://github.com/pretto0)       |
| [MORAN](./configs/rec/moran/)                 | [PR 2019](https://www.sciencedirect.com/science/article/abs/pii/S0031320319300263)               | âœ…       | âœ…       |                                             |
| [DAN](./configs/rec/dan/)                     | [AAAI 2020](https://arxiv.org/pdf/1912.10205)                                                    | âœ…       | âœ…       |                                             |
| [RobustScanner](./configs/rec/robustscanner/) | [ECCV 2020](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3160_ECCV_2020_paper.php)     | âœ…       | âœ…       | [pretto0](https://github.com/pretto0)       |
| [AutoSTR](./configs/rec/autostr/)             | [ECCV 2020](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690732.pdf)              | âœ…       | âœ…       |                                             |
| [SRN](./configs/rec/srn/)                     | [CVPR 2020](https://openaccess.thecvf.com/content_CVPR_2020/html/Yu_Towards_Accurate_Scene_Text_Recognition_With_Semantic_Reasoning_Networks_CVPR_2020_paper.html) | âœ…       | âœ…       | [pretto0](https://github.com/pretto0)       |
| [SEED](./configs/rec/seed/)                   | [CVPR 2020](https://openaccess.thecvf.com/content_CVPR_2020/html/Qiao_SEED_Semantics_Enhanced_Encoder-Decoder_Framework_for_Scene_Text_Recognition_CVPR_2020_paper.html) | âœ…       | âœ…       |                                             |
| [ABINet](./configs/rec/abinet/)               | [CVPR 2021](https://openaccess.thecvf.com//content/CVPR2021/html/Fang_Read_Like_Humans_Autonomous_Bidirectional_and_Iterative_Language_Modeling_for_CVPR_2021_paper.html) | âœ…       | âœ…       | [YesianRohn](https://github.com/YesianRohn) |
| [VisionLAN](./configs/rec/visionlan/)         | [ICCV 2021](https://openaccess.thecvf.com/content/ICCV2021/html/Wang_From_Two_to_One_A_New_Scene_Text_Recognizer_With_ICCV_2021_paper.html) | âœ…       | âœ…       | [YesianRohn](https://github.com/YesianRohn) |
| PIMNet                                        | [ACM MM 2021](https://dl.acm.org/doi/10.1145/3474085.3475238)                                    |          |          | TODO                                        |
| [SVTR](./configs/rec/svtrs/)                  | [IJCAI 2022](https://www.ijcai.org/proceedings/2022/124)                                         | âœ…       | âœ…       |                                             |
| [PARSeq](./configs/rec/parseq/)               | [ECCV 2022](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880177.pdf)              | âœ…       | âœ…       |                                             |
| [MATRN](./configs/rec/matrn/)                 | [ECCV 2022](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880442.pdf)              | âœ…       | âœ…       |                                             |
| [MGP-STR](./configs/rec/mgpstr/)              | [ECCV 2022](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880336.pdf)              | âœ…       | âœ…       |                                             |
| [LPV](./configs/rec/lpv/)                     | [IJCAI 2023](https://www.ijcai.org/proceedings/2023/0189.pdf)                                    | âœ…       | âœ…       |                                             |
| [MAERec](./configs/rec/maerec/)(Union14M)     | [ICCV 2023](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Revisiting_Scene_Text_Recognition_A_Data_Perspective_ICCV_2023_paper.pdf) | âœ…       | âœ…       |                                             |
| [LISTER](./configs/rec/lister/)               | [ICCV 2023](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_LISTER_Neighbor_Decoding_for_Length-Insensitive_Scene_Text_Recognition_ICCV_2023_paper.pdf) | âœ…       | âœ…       |                                             |
| [CDistNet](./configs/rec/cdistnet/)           | [IJCV 2024](https://link.springer.com/article/10.1007/s11263-023-01880-0)                        | âœ…       | âœ…       | [YesianRohn](https://github.com/YesianRohn) |
| [BUSNet](./configs/rec/busnet/)               | [AAAI 2024](https://ojs.aaai.org/index.php/AAAI/article/view/28402)                              | âœ…       | âœ…       |                                             |
| DCTC                                          | [AAAI 2024](https://ojs.aaai.org/index.php/AAAI/article/view/28575)                              |          |          | TODO                                        |
| [CAM](./configs/rec/cam/)                     | [PR 2024](https://arxiv.org/abs/2402.13643)                                                      | âœ…       | âœ…       |                                             |
| [OTE](./configs/rec/ote/)                     | [CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/html/Xu_OTE_Exploring_Accurate_Scene_Text_Recognition_Using_One_Token_CVPR_2024_paper.html) | âœ…       | âœ…       |                                             |
| CFF                                           | [IJCAI 2024](https://arxiv.org/abs/2407.05562)                                                   |          |          | TODO                                        |
| [DPTR](./configs/rec/dptr/)                   | [ACM MM 2024](https://arxiv.org/abs/2408.05706)                                                  |          |          | [fd-zs](https://github.com/fd-zs)           |
| VIPTR                                         | [ACM CIKM 2024](https://arxiv.org/abs/2401.10110)                                                |          |          | TODO                                        |
| [IGTR](./configs/rec/igtr/)                   | [TPAMI 2025](https://ieeexplore.ieee.org/document/10820836)                                      | âœ…       | âœ…       |                                             |
| [SMTR](./configs/rec/smtr/)                   | [AAAI 2025](https://arxiv.org/abs/2407.12317)                                                    | âœ…       | âœ…       |                                             |
| [CPPD](./configs/rec/cppd/)                   | [TPAMI Online Access](https://ieeexplore.ieee.org/document/10902187)                             | âœ…       | âœ…       |                                             |
| [FocalSVTR-CTC](./configs/rec/svtrs/)         | [2024](https://arxiv.org/abs/2407.12317)                                                         | âœ…       | âœ…       |                                             |
| [SVTRv2](./configs/rec/svtrv2/)               | [2024](https://arxiv.org/abs/2411.15858)                                                         | âœ…       | âœ…       |                                             |
| [ResNet+Trans-CTC](./configs/rec/svtrs/)      |                                                                                                  | âœ…       | âœ…       |                                             |
| [ViT-CTC](./configs/rec/svtrs/)               |                                                                                                  | âœ…       | âœ…       |                                             |

#### æ ¸å¿ƒè´¡çŒ®è€…

______________________________________________________________________

å¤æ—¦å¤§å­¦[FVLå®éªŒå®¤](https://fvl.fudan.edu.cn)çš„Yiming Lei ([pretto0](https://github.com/pretto0)), Xingsong Ye ([YesianRohn](https://github.com/YesianRohn)), and Shuai Zhao ([fd-zs](https://github.com/fd-zs))åœ¨Zhineng Chenè€å¸ˆ([ä¸ªäººä¸»é¡µ](https://zhinchenfd.github.io/))æŒ‡å¯¼ä¸‹å®Œæˆäº†ä¸»è¦ç®—æ³•å¤ç°å·¥ä½œï¼Œæ„Ÿè°¢ä»–ä»¬çš„è´¡çŒ®ã€‚

### åœºæ™¯æ–‡æœ¬æ£€æµ‹(STD)

å¼€å‘ä¸­

### ç«¯åˆ°ç«¯æ–‡æœ¬è¯†åˆ«(Text Spotting)

å¼€å‘ä¸­

______________________________________________________________________

## å¼•ç”¨

å¦‚æœæˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨çš„ç ”ç©¶æœ‰æ‰€å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@inproceedings{Du2024SVTRv2,
      title={SVTRv2: CTC Beats Encoder-Decoder Models in Scene Text Recognition},
      author={Yongkun Du and Zhineng Chen and Hongtao Xie and Caiyan Jia and Yu-Gang Jiang},
      booktitle={ICCV},
      year={2025}
}
```

## è‡´è°¢

æœ¬ä»£ç åº“åŸºäº[PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)ã€[PytorchOCR](https://github.com/WenmuZhou/PytorchOCR)å’Œ[MMOCR](https://github.com/open-mmlab/mmocr)æ„å»ºï¼Œæ„Ÿè°¢ä»–ä»¬çš„å‡ºè‰²å·¥ä½œï¼
