import gradio as gr
import torch
from threading import Thread

import numpy as np
from openrec.postprocess import build_post_process
from openrec.preprocess import create_operators, transform
from tools.engine.config import Config
from tools.utils.ckpt import load_ckpt
from tools.infer_rec import build_rec_process


def set_device(device):
    if device == 'gpu' and torch.cuda.is_available():
        device = torch.device(f'cuda:0')
    else:
        device = torch.device('cpu')
    return device


cfg = Config('configs/rec/unirec/focalsvtr_ardecoder_unirec.yml')
cfg = cfg.cfg
global_config = cfg['Global']

# build post process
post_process_class = build_post_process(cfg['PostProcess'], cfg['Global'])

from openrec.modeling.transformers_modeling.modeling_unirec import UniRecForConditionalGenerationNew
from openrec.modeling.transformers_modeling.configuration_unirec import UniRecConfig
from transformers import AutoTokenizer, TextIteratorStreamer

tokenizer = AutoTokenizer.from_pretrained('./configs/rec/unirec/unirec_100m')
cfg_model = UniRecConfig.from_pretrained('./configs/rec/unirec/unirec_100m')
# cfg_model._attn_implementation = "flash_attention_2"
cfg_model._attn_implementation = 'eager'

model = UniRecForConditionalGenerationNew(config=cfg_model)
load_ckpt(model, cfg)
device = set_device(cfg['Global']['device'])
model.eval()
model.to(device=device)

transforms, ratio_resize_flag = build_rec_process(cfg)
ops = create_operators(transforms, global_config)


# --- 2. å®šä¹‰æµå¼ç”Ÿæˆå‡½æ•° ---
def stream_chat_with_image(input_image, history):
    if input_image is None:
        yield history + [('ğŸ–¼ï¸(ç©º)', 'è¯·å…ˆä¸Šä¼ ä¸€å¼ å›¾ç‰‡ã€‚')]
        return

    # åˆ›å»º TextIteratorStreamer
    streamer = TextIteratorStreamer(tokenizer,
                                    skip_prompt=True,
                                    skip_special_tokens=False)

    data = {'image': input_image}
    batch = transform(data, ops[1:])
    images = np.expand_dims(batch[0], axis=0)
    images = torch.from_numpy(images).to(device=device)
    inputs = {
        'pixel_values': images,
        'input_ids': None,
        'attention_mask': None
    }
    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=1024)
    # åå°çº¿ç¨‹è¿è¡Œç”Ÿæˆ
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    # æµå¼è¾“å‡º
    generated_text = ''
    history = history + [('ğŸ–¼ï¸(å›¾ç‰‡)', '')]
    for new_text in streamer:
        new_text = new_text.replace(' ', '').replace('Ä ', ' ').replace(
            'ÄŠ', '\n').replace('<|bos|>',
                               '').replace('<|eos|>',
                                           '').replace('<|pad|>', '')
        generated_text += new_text
        history[-1] = ('ğŸ–¼ï¸(å›¾ç‰‡)', generated_text)
        yield history


# --- 3. Gradio ç•Œé¢ ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.HTML("""
            <h1 style='text-align: center;'><a href="https://github.com/Topdu/OpenOCR">UniRec: Unified Text and Formula Recognition Across Granularities</a></h1>
            <p style='text-align: center;'>ç»Ÿä¸€å¤šç²’åº¦æ–‡æœ¬ä¸å…¬å¼è¯†åˆ«æ¨¡å‹ ï¼ˆç”±<a href="https://fvl.fudan.edu.cn">FVLå®éªŒå®¤</a> <a href="https://github.com/Topdu/OpenOCR">OCR Team</a> åˆ›å»ºï¼‰</p>
            <p style='text-align: center;'><a href="https://github.com/Topdu/OpenOCR/blob/openocr_svtrv2/docs/unirec.md">[æœ¬åœ°GPUéƒ¨ç½²]</a>è·å–å¿«é€Ÿè¯†åˆ«ä½“éªŒ</p>"""
            )
    gr.Markdown('ä¸Šä¼ ä¸€å¼ å›¾ç‰‡ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«æ–‡æœ¬å’Œå…¬å¼ã€‚')
    with gr.Row():
        with gr.Column(scale=1):  # å·¦ä¾§ç«–æ’ï¼šå›¾ç‰‡ + æ¸…ç©ºæŒ‰é’®
            image_input = gr.Image(label='ä¸Šä¼ å›¾ç‰‡ or ç²˜è´´æˆªå›¾', type='pil')
            clear = gr.ClearButton([image_input],
                                   value='æ¸…ç©º')  # å…ˆæŒ‚è½½åˆ° image_input
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(label='ç»“æœï¼ˆè¯·ä½¿ç”¨LaTeXç¼–è¯‘å™¨æ¸²æŸ“å…¬å¼ï¼‰',
                                 show_copy_button=True,
                                 height='auto')
    # å†æŠŠ clear ç»‘å®š chatbot ä¸€èµ·æ¸…ç†
    clear.add([chatbot])
    # ä¸Šä¼ åè§¦å‘
    image_input.upload(stream_chat_with_image, [image_input, chatbot], chatbot)

# --- 4. å¯åŠ¨åº”ç”¨ ---
if __name__ == '__main__':
    demo.queue().launch(share=True)
