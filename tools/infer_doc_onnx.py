from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from pathlib import Path

import os
import json
import time
import argparse
from typing import Dict, Optional, Union, List, Tuple

import cv2
import numpy as np
from PIL import Image
import onnxruntime as ort
from tools.utils.logging import get_logger
from tools.utils.utility import get_image_file_list

from tools.utils.opendoc_onnx_utils.utils import (
    convert_otsl_to_html,
    crop_margin,
    filter_overlap_boxes,
    merge_blocks,
    tokenize_figure_of_table,
    truncate_repetitive_content,
    untokenize_figure_of_table,
)
from tools.to_markdown import MarkdownConverter
from tools.infer_unirec_onnx import (
    UniRecONNX
)

# åˆ›å»ºå…¨å±€ markdown_converter å®ä¾‹
markdown_converter = MarkdownConverter()

logger = get_logger(name='opendoc_onnx')

root_dir = Path(__file__).resolve().parent

IMAGE_LABELS = ['image', 'header_image', 'footer_image', 'seal']


def download_layout_model(model_dir=None):
    """Download layout detection ONNX model from ModelScope or HuggingFace.

    Args:
        model_dir: Directory to save model file. If None, use default cache directory.

    Returns:
        Path to the downloaded model file
    """
    # Use default cache directory if not specified
    if model_dir is None:
        cache_dir = Path.home() / '.cache' / 'openocr'
        model_dir = cache_dir / 'PP_DoclayoutV2_onnx'
    else:
        model_dir = Path(model_dir)

    model_dir.mkdir(parents=True, exist_ok=True)

    model_file = 'PP-DoclayoutV2.onnx'
    model_path = model_dir / model_file

    # Check if model already exists
    if model_path.exists():
        logger.info(f'âœ… Layout model found in {model_dir}')
        return str(model_path)

    logger.info(f'ğŸ“¥ Downloading layout model to {model_dir}...')

    download_success = False

    try:
        # Try ModelScope first (default)
        logger.info('ğŸŒ Trying ModelScope (China mirror) first...')
        try:
            from modelscope import snapshot_download
            downloaded_path = snapshot_download(
                'topdktu/PP_DoclayoutV2_onnx',
                cache_dir=str(model_dir.parent)
            )
            logger.info(f'âœ… Downloaded to {downloaded_path}')

            # Copy file to target directory
            import shutil
            src = Path(downloaded_path) / model_file
            if src.exists() and not model_path.exists():
                shutil.copy(str(src), str(model_path))
                logger.info(f'  âœ“ {model_file}')

            # Verify file exists after download
            if model_path.exists():
                download_success = True
                logger.info('âœ… Layout model downloaded successfully from ModelScope!')
            else:
                logger.info('âš ï¸  ModelScope download incomplete, trying HuggingFace...')

        except ImportError:
            logger.info('ModelScope not installed. Install with: pip install modelscope')
            logger.info('Trying HuggingFace...')
        except Exception as e:
            logger.info(f'ModelScope download failed: {e}')
            logger.info('Trying HuggingFace...')

        if not download_success:
            # Try HuggingFace
            logger.info('ğŸŒ Using HuggingFace...')
            try:
                from huggingface_hub import hf_hub_download
                logger.info(f'  Downloading {model_file}...')
                downloaded_path = hf_hub_download(
                    repo_id='topdu/PP_DoclayoutV2_onnx',
                    filename=model_file,
                    cache_dir=str(model_dir.parent),
                    local_dir=str(model_dir),
                    local_dir_use_symlinks=False
                )
                logger.info(f'  âœ“ {model_file}')

                # Verify file exists after download
                if model_path.exists():
                    download_success = True
                    logger.info('âœ… Layout model downloaded successfully from HuggingFace!')

            except ImportError:
                raise ImportError('HuggingFace Hub not installed. Install with: pip install huggingface_hub')

        if not download_success:
            raise RuntimeError(
                'Failed to download layout model. Please manually download from:\n'
                '  - https://huggingface.co/topdu/PP_DoclayoutV2_onnx\n'
                '  - https://modelscope.cn/models/topdktu/PP_DoclayoutV2_onnx'
            )

    except Exception as e:
        logger.error(f'âŒ Failed to download layout model: {e}')
        raise

    return str(model_path)


def check_and_download_layout_model(model_path, auto_download=True):
    """Check if layout model exists, download if missing.

    Args:
        model_path: Path to layout model file
        auto_download: If True, automatically download missing model

    Returns:
        Path to the model file
    """
    if model_path and os.path.exists(model_path):
        return model_path

    if not auto_download:
        if not model_path or not os.path.exists(model_path):
            logger.error(f'âš ï¸  Layout model not found: {model_path}')
            logger.info('\nğŸ“ Manual download instructions:')
            logger.info('   1. Visit: https://huggingface.co/topdu/PP_DoclayoutV2_onnx')
            logger.info('   2. Download PP-DoclayoutV2.onnx')
            logger.info('   3. Specify path with --layout_model argument')
            raise FileNotFoundError(f'Layout model not found: {model_path}')

    # Determine model directory from model path
    default_path = str(Path.home() / '.cache' / 'openocr' / 'PP_DoclayoutV2_onnx' / 'PP-DoclayoutV2.onnx')
    if model_path and model_path != default_path:
        # User specified a custom path
        model_dir = os.path.dirname(model_path)
    else:
        # Use default cache directory
        model_dir = None

    # Try ModelScope first (faster in China), then HuggingFace
    try:
        logger.info('ğŸ‡¨ğŸ‡³ Trying ModelScope (China mirror) first...')
        return download_layout_model(model_dir)
    except:
        logger.info('ğŸŒ Trying HuggingFace...')
        return download_layout_model(model_dir)


def _get_image_name_and_dir(result: Dict, output_path: str):
    """æ ¹æ®å›¾ç‰‡ååˆ›å»ºå­ç›®å½•å¹¶è¿”å›(img_name, img_dir)"""
    img_name = os.path.basename(result['input_path'])
    if '.' in img_name:
        img_name = img_name.rsplit('.', 1)[0]

    img_dir = os.path.join(output_path, img_name)
    os.makedirs(img_dir, exist_ok=True)

    return img_name, img_dir


# ==================== Layout Detection ONNX ====================
class LayoutDetectorONNX:
    """ONNXç‰ˆæœ¬çš„ç‰ˆé¢æ£€æµ‹æ¨¡å‹"""

    def __init__(self,
                 model_path: str,
                 use_gpu: Optional[bool] = None,
                 threshold: float = 0.5,
                 auto_download: bool = True):
        """
        åˆå§‹åŒ–ONNXç‰ˆé¢æ£€æµ‹æ¨¡å‹

        Args:
            model_path: ONNXæ¨¡å‹è·¯å¾„
            use_gpu: Whether to use GPU. If None, auto-detect. If True, force GPU. If False, force CPU.
            threshold: æ£€æµ‹é˜ˆå€¼
            auto_download: If True, automatically download missing model
        """
        self.threshold = threshold

        # Check and download model if needed
        model_path = check_and_download_layout_model(model_path, auto_download=auto_download)

        # Determine execution providers
        providers = self._get_execution_providers(use_gpu)
        logger.info(f'Layout detector using: {providers[0]}')

        # åˆ›å»ºONNX Runtimeä¼šè¯
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        self.session = ort.InferenceSession(model_path,
                                            sess_options,
                                            providers=providers)

        # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
        self.input_names = [inp.name for inp in self.session.get_inputs()]
        self.output_names = [
            output.name for output in self.session.get_outputs()
        ]

        logger.info(f"   Input names: {self.input_names}")
        logger.info(f"   Output names: {self.output_names}")

        self.label_map = {
            0: 'abstract',
            1: 'algorithm',
            2: 'aside_text',
            3: 'chart',
            4: 'content',
            5: 'display_formula',
            6: 'doc_title',
            7: 'figure_title',
            8: 'footer',
            9: 'footer_image',
            10: 'footnote',
            11: 'formula_number',
            12: 'header',
            13: 'header_image',
            14: 'image',
            15: 'inline_formula',
            16: 'number',
            17: 'paragraph_title',
            18: 'reference',
            19: 'reference_content',
            20: 'seal',
            21: 'table',
            22: 'text',
            23: 'vertical_text',
            24: 'vision_footnote'
        }

    def _get_execution_providers(self, use_gpu):
        """Determine execution providers based on GPU availability and user preference.

        Args:
            use_gpu: None (auto-detect), True (force GPU), or False (force CPU)

        Returns:
            List of execution providers in priority order
        """
        available_providers = ort.get_available_providers()

        if use_gpu is False:
            # Force CPU
            logger.info('ğŸ”§ User specified: Using CPU for layout detection')
            return ['CPUExecutionProvider']

        # Check for GPU providers
        gpu_providers = []
        if 'TensorrtExecutionProvider' in available_providers:
            gpu_providers.append('TensorrtExecutionProvider')
        if 'CUDAExecutionProvider' in available_providers:
            gpu_providers.append('CUDAExecutionProvider')

        if use_gpu is True:
            # Force GPU
            if gpu_providers:
                logger.info(f'ğŸ”§ User specified: Using GPU for layout detection ({gpu_providers[0]})')
                return gpu_providers + ['CPUExecutionProvider']
            else:
                logger.warning('âš ï¸  GPU requested but not available, falling back to CPU')
                return ['CPUExecutionProvider']

        # Auto-detect (use_gpu is None)
        if gpu_providers:
            logger.info(f'âœ… GPU detected for layout detection: Using {gpu_providers[0]}')
            return gpu_providers + ['CPUExecutionProvider']
        else:
            logger.info('â„¹ï¸  No GPU detected for layout detection, using CPU')
            return ['CPUExecutionProvider']



    def crop_by_boxes(self, image: np.ndarray,
                      boxes: List[Dict]) -> List[Dict]:
        """
        æ ¹æ®æ£€æµ‹æ¡†è£å‰ªå›¾åƒåŒºåŸŸ

        Args:
            image: BGRæ ¼å¼çš„åŸå§‹å›¾åƒ
            boxes: æ£€æµ‹æ¡†åˆ—è¡¨

        Returns:
            åŒ…å«è£å‰ªå›¾åƒçš„å—åˆ—è¡¨
        """
        blocks = []
        for box in boxes:
            coord = box['coordinate']
            x1, y1, x2, y2 = map(int, coord)

            # è£å‰ªå›¾åƒ
            cropped_img = image[y1:y2, x1:x2]
            if cropped_img.size == 0:
                cropped_img = None

            blocks.append({
                'img': cropped_img,
                'box': coord,
                'label': box['label'],
                'score': box.get('score', 1.0),
                'cls_id': box.get('cls_id', -1),
                'custom_value': box.get('custom_value', 0),
            })
        return blocks

    def preprocess(
        self, image: np.ndarray, target_input_size: tuple = (800, 800)
    ) -> Tuple[Dict, Tuple[float, float], int, int]:
        """
        Args:
            image: BGRæ ¼å¼çš„å›¾åƒ
            target_input_size: ç›®æ ‡å°ºå¯¸ (height, width)

        Returns:
            è¾“å…¥å­—å…¸, (scale_h, scale_w), åŸå§‹é«˜åº¦, åŸå§‹å®½åº¦
        """
        # Get original dimensions
        orig_h, orig_w = image.shape[:2]

        #  Resize (keep_ratio=false, interp=2)
        target_h, target_w = target_input_size
        scale_h = target_h / orig_h
        scale_w = target_w / orig_w

        new_h, new_w = int(orig_h * scale_h), int(orig_w * scale_w)
        resized = cv2.resize(image, (new_w, new_h),
                             interpolation=cv2.INTER_LINEAR)

        # Convert BGR to RGB
        resized_rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)

        input_blob = resized_rgb.astype(np.float32) / 255.0

        input_blob = input_blob.transpose(2, 0, 1)[np.newaxis, ...]

        preprocess_shape = np.array([[target_h, target_w]], dtype=np.float32)

        # scale_factor: [[scale_h, scale_w]]
        scale_factor = np.array([[scale_h, scale_w]], dtype=np.float32)

        inputs = {
            'im_shape': preprocess_shape,  # shape: [1, 2]
            'image': input_blob.astype(np.float32),
            'scale_factor': scale_factor  # shape: [1, 2]
        }

        return inputs, (scale_h, scale_w), orig_h, orig_w

    def postprocess(
        self,
        image: np.ndarray,
        outputs: list,
        scale: Tuple[float, float],
        ori_h: int,
        ori_w: int,
        merge_layout_blocks: bool = True,
        use_chart_recognition: bool = False,
    ) -> Dict:
        """
        åå¤„ç†ï¼Œä»¿ç…§ get_layout_parsing_results çš„é€»è¾‘

        Args:
            image: åŸå§‹å›¾åƒ (BGRæ ¼å¼)
            outputs: æ¨¡å‹è¾“å‡º
            scale: ç¼©æ”¾å› å­ (scale_h, scale_w)
            ori_h: åŸå§‹é«˜åº¦
            ori_w: åŸå§‹å®½åº¦
            merge_layout_blocks: æ˜¯å¦åˆå¹¶å¸ƒå±€å—
            use_chart_recognition: æ˜¯å¦è¯†åˆ«å›¾è¡¨

        Returns:
            æ£€æµ‹ç»“æœå­—å…¸ï¼ŒåŒ…å« boxes å’Œ blocks
        """
        # PaddleDetection ONNX è¾“å‡ºæ ¼å¼:
        # outputs[0]: bbox [N, 8] - å‰6ä¸ªå€¼: [class_id, score, x1, y1, x2, y2]
        bboxes = outputs[0]  # [N, 8]

        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•æ¡†
        if bboxes.shape[0] == 0:
            return {'boxes': [], 'blocks': []}

        # è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„æ¡†
        filtered_bboxes = bboxes[bboxes[:, 1] > self.threshold]

        if filtered_bboxes.shape[0] == 0:
            return {'boxes': [], 'blocks': []}

        # è§£ææ¯ä¸ªæ£€æµ‹æ¡†
        result_boxes = []
        for bbox in filtered_bboxes:
            class_id = int(bbox[0])
            score = float(bbox[1])
            order_value = float(bbox[6])
            x1, y1, x2, y2 = bbox[2:6]

            # è£å‰ªåˆ°å›¾åƒè¾¹ç•Œ
            x1 = float(np.clip(x1, 0, ori_w))
            y1 = float(np.clip(y1, 0, ori_h))
            x2 = float(np.clip(x2, 0, ori_w))
            y2 = float(np.clip(y2, 0, ori_h))

            result_boxes.append({
                'cls_id':
                class_id,
                'label':
                self.label_map.get(class_id, f'class_{class_id}'),
                'score':
                score,
                'coordinate': [x1, y1, x2, y2],
                'custom_value':
                order_value
            })

        result_dict = {'boxes': result_boxes}

        # å»é™¤é‡å æ¡†
        result_dict = filter_overlap_boxes(result_dict)

        # æ ¹æ® custom_value æ’åº
        result_dict['boxes'] = sorted(result_dict['boxes'],
                                      key=lambda box: box['custom_value'],
                                      reverse=False)

        # ç»™æ¯ä¸ª label æ·»åŠ é¡ºåºç¼–å·
        for idx, box in enumerate(result_dict['boxes'], start=1):
            base_label = box['label']
            box['label'] = f"{base_label}_{idx:02d}"

        # è£å‰ªå›¾åƒåŒºåŸŸ
        blocks = self.crop_by_boxes(image, result_dict['boxes'])

        # ç¡®å®š image_labels
        image_labels = IMAGE_LABELS if use_chart_recognition else IMAGE_LABELS + [
            'chart'
        ]

        # åˆå¹¶å¸ƒå±€å—
        if merge_layout_blocks:
            blocks = merge_blocks(blocks,
                                  non_merge_labels=image_labels + ['table'])

        result_dict['blocks'] = blocks

        return result_dict

    def __call__(self,
                 images: Union[np.ndarray, List[np.ndarray]],
                 threshold: Optional[float] = None) -> List[Dict]:
        """
        æ‰§è¡Œç‰ˆé¢æ£€æµ‹

        Args:
            images: å•å¼ æˆ–å¤šå¼ å›¾åƒ
            threshold: ç½®ä¿¡åº¦é˜ˆå€¼

        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        if threshold is not None:
            original_threshold = self.threshold
            self.threshold = threshold

        if isinstance(images, np.ndarray):
            images = [images]

        results = []
        for image in images:
            # é¢„å¤„ç†
            input_dict, scale, ori_h, ori_w = self.preprocess(image)

            # æ¨ç†
            outputs = self.session.run(self.output_names, input_dict)

            # åå¤„ç†
            result = self.postprocess(image, outputs, scale, ori_h, ori_w)
            results.append(result)

        if threshold is not None:
            self.threshold = original_threshold

        return results



# ==================== OpenDoc ONNX Pipeline ====================
class OpenDocONNX:
    """å®Œæ•´çš„æ–‡æ¡£OCR ONNX Pipeline"""

    def __init__(
        self,
        layout_model_path: Optional[str] = None,
        unirec_encoder_path: Optional[str] = None,
        unirec_decoder_path: Optional[str] = None,
        tokenizer_mapping_path: Optional[str] = None,
        use_gpu: Optional[bool] = None,
        layout_threshold: float = 0.5,
        use_layout_detection: bool = True,
        use_chart_recognition: bool = True,
        auto_download: bool = True,
    ):
        """
        åˆå§‹åŒ–OpenDoc ONNX Pipeline

        Args:
            layout_model_path: ç‰ˆé¢æ£€æµ‹ONNXæ¨¡å‹è·¯å¾„. If None, use default cache directory.
            unirec_encoder_path: UniRecç¼–ç å™¨ONNXæ¨¡å‹è·¯å¾„. If None, use default cache directory.
            unirec_decoder_path: UniRecè§£ç å™¨ONNXæ¨¡å‹è·¯å¾„. If None, use default cache directory.
            tokenizer_mapping_path: Tokenizeræ˜ å°„æ–‡ä»¶è·¯å¾„. If None, use default cache directory.
            use_gpu: Whether to use GPU. If None, auto-detect. If True, force GPU. If False, force CPU.
            layout_threshold: ç‰ˆé¢æ£€æµ‹é˜ˆå€¼
            use_layout_detection: æ˜¯å¦ä½¿ç”¨ç‰ˆé¢æ£€æµ‹
            use_chart_recognition: æ˜¯å¦è¯†åˆ«å›¾è¡¨
            auto_download: If True, automatically download missing models
        """
        self.use_layout_detection = use_layout_detection
        self.use_chart_recognition = use_chart_recognition

        # Set default paths if not provided
        if layout_model_path is None:
            cache_dir = Path.home() / '.cache' / 'openocr'
            layout_model_path = str(cache_dir / 'PP_DoclayoutV2_onnx' / 'PP-DoclayoutV2.onnx')

        # Markdownå¿½ç•¥çš„æ ‡ç­¾
        self.markdown_ignore_labels = [
            'number', 'footnote', 'header', 'footer', 'aside_text', 'footer_image', 'header_image','chart'
        ]

        # ä¸ºæ‰€æœ‰25ç§æ ‡ç­¾ç±»å‹å®šä¹‰ä¸åŒçš„é¢œè‰² (BGRæ ¼å¼)
        self.colors = {
            'abstract': (255, 128, 0),        # æ©™è‰²
            'algorithm': (128, 0, 255),       # ç´«è‰²
            'aside_text': (128, 128, 128),    # ç°è‰²
            'chart': (0, 255, 255),           # é’è‰²
            'content': (0, 255, 0),           # ç»¿è‰²
            'display_formula': (255, 0, 255), # å“çº¢
            'doc_title': (255, 0, 0),         # çº¢è‰²
            'figure_title': (255, 128, 128),  # æµ…çº¢
            'footer': (64, 64, 64),           # æ·±ç°
            'footer_image': (128, 64, 0),     # æ£•è‰²
            'footnote': (192, 192, 192),      # æµ…ç°
            'formula_number': (255, 128, 255),# æµ…å“çº¢
            'header': (96, 96, 96),           # ä¸­ç°
            'header_image': (0, 128, 128),    # æ·±é’
            'image': (0, 255, 255),           # é’è‰²
            'inline_formula': (200, 0, 200),  # æ·±å“çº¢
            'number': (128, 255, 0),          # é»„ç»¿
            'paragraph_title': (255, 64, 0),  # æ©™çº¢
            'reference': (0, 128, 255),       # å¤©è“
            'reference_content': (128, 192, 255), # æµ…è“
            'seal': (0, 0, 128),              # æ·±è“
            'table': (0, 0, 255),             # è“è‰²
            'text': (0, 200, 0),              # æ·±ç»¿
            'vertical_text': (128, 255, 128), # æµ…ç»¿
            'vision_footnote': (160, 160, 160) # ä¸­æµ…ç°
        }

        # åˆå§‹åŒ–ç‰ˆé¢æ£€æµ‹æ¨¡å‹
        if use_layout_detection:
            self.layout_detector = LayoutDetectorONNX(
                layout_model_path, use_gpu=use_gpu, threshold=layout_threshold, auto_download=auto_download)
        else:
            self.layout_detector = None

        # åˆå§‹åŒ–VLMæ¨¡å‹
        self.vlm_recognizer = UniRecONNX(
            encoder_path=unirec_encoder_path,
            decoder_path=unirec_decoder_path,
            mapping_path=tokenizer_mapping_path,
            use_gpu=use_gpu,
            auto_download=auto_download)


    def __call__(
        self,
        img_path: Optional[str] = None,
        img_numpy: Optional[np.ndarray] = None,
        image_path: Optional[str] = None,
        layout_threshold: Optional[float] = None,
        max_length: int = 2048,
        merge_layout_blocks: bool = True,
    ) -> Dict:
        """
        Unified interface for OpenDoc inference.

        Args:
            img_path: Path to input image (str or Path)
            img_numpy: Input image as numpy array (BGR format)
            image_path: Alias for img_path (for backward compatibility)
            layout_threshold: Layout detection threshold
            max_length: VLM maximum generation length
            merge_layout_blocks: Whether to merge layout blocks

        Returns:
            Prediction result dictionary
        """
        # Handle backward compatibility: image_path is alias for img_path
        if image_path is not None and img_path is None:
            img_path = image_path

        # Load image from path or numpy array
        is_temp_file = False
        if img_path is not None:
            actual_path = img_path
        elif img_numpy is not None:
            # For numpy array input, we need to save it temporarily
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp_file:
                tmp_path = tmp_file.name
                cv2.imwrite(tmp_path, img_numpy)
            actual_path = tmp_path
            is_temp_file = True
        else:
            raise ValueError('Either img_path or img_numpy must be provided')

        start_time = time.time()

        # è¯»å–å›¾åƒ
        image = cv2.imread(actual_path)
        if image is None:
            raise ValueError(f"Failed to read image: {actual_path}")

        ori_h, ori_w = image.shape[:2]

        # ç‰ˆé¢æ£€æµ‹
        layout_results = None
        if self.use_layout_detection:
            layout_results = self.layout_detector(
                [image], threshold=layout_threshold)[0]
        else:
            # æ•´å¼ å›¾ä½œä¸ºä¸€ä¸ªåŒºåŸŸ
            layout_results = {
                'boxes': [{
                    'cls_id': 0,
                    'label': 'text',
                    'score': 1.0,
                    'coordinate': [0, 0, ori_w, ori_h]
                }]
            }
            logger.info('  Layout detection disabled, processing whole image')

        # ç¡®å®š image_labels
        image_labels = (IMAGE_LABELS if self.use_chart_recognition else
                        IMAGE_LABELS + ['chart'])

        # è£å‰ªå›¾åƒåŒºåŸŸå¹¶åˆå¹¶å¸ƒå±€å—
        boxes = layout_results['boxes']
        blocks = []
        for box in boxes:
            coord = box['coordinate']
            x1, y1, x2, y2 = map(int, coord)
            cropped_img = image[y1:y2, x1:x2]
            if cropped_img.size == 0:
                cropped_img = None
            blocks.append({
                'img': cropped_img,
                'box': coord,
                'label': box['label'],
                'score': box.get('score', 1.0),
            })

        # åˆå¹¶å¸ƒå±€å—
        if merge_layout_blocks:
            blocks = merge_blocks(blocks,
                                  non_merge_labels=image_labels + ['table'])

        # æ”¶é›†éœ€è¦VLMå¤„ç†çš„blocks
        block_imgs = []
        text_prompts = []
        block_labels = []
        vlm_block_ids = []
        figure_token_maps = []
        drop_figures_set = set()
        imgs_in_doc = []  # å½“å‰å›¾åƒä¸­çš„å›¾ç‰‡åŒºåŸŸ

        for j, block in enumerate(blocks):
            block_label = block['label']
            # æå–åŸºç¡€æ ‡ç­¾åï¼ˆå»é™¤ç¼–å·åç¼€ï¼‰
            base_label = block_label.rsplit(
                '_', 1)[0] if '_' in block_label and block_label.rsplit(
                    '_', 1)[1].isdigit() else block_label
            if base_label in image_labels and block['img'] is not None:
                x_min, y_min, x_max, y_max = list(map(int, block['box']))
                img_path = f'imgs/img_in_{base_label}_box_{x_min}_{y_min}_{x_max}_{y_max}.jpg'
                imgs_in_doc.append({
                    'coordinate': block['box'],
                    'path': img_path
                })

        # å¤„ç†æ¯ä¸ªblock
        for j, block in enumerate(blocks):
            block_img = block['img']
            block_label = block['label']
            # æå–åŸºç¡€æ ‡ç­¾åï¼ˆå»é™¤ç¼–å·åç¼€ï¼‰
            base_label = block_label.rsplit(
                '_', 1)[0] if '_' in block_label and block_label.rsplit(
                    '_', 1)[1].isdigit() else block_label

            if base_label not in image_labels and block_img is not None:
                figure_token_map = {}
                text_prompt = 'OCR:'
                drop_figures = []

                if 'table' in block_label:
                    text_prompt = 'Table Recognition:'
                    block_img, figure_token_map, drop_figures = (
                        tokenize_figure_of_table(block_img, block['box'],
                                                 imgs_in_doc))
                elif block_label == 'chart' and self.use_chart_recognition:
                    text_prompt = 'Chart Recognition:'
                elif 'formula' in block_label and block_label != 'formula_number':
                    text_prompt = 'Formula Recognition:'
                    block_img = crop_margin(block_img)

                block_imgs.append(block_img)
                text_prompts.append(text_prompt)
                block_labels.append(block_label)
                figure_token_maps.append(figure_token_map)
                vlm_block_ids.append(j)
                drop_figures_set.update(drop_figures)

        # VLMè¯†åˆ«
        vl_rec_results = []

        for block_img, block_label in zip(block_imgs, block_labels):
            # è½¬æ¢ä¸ºRGB PIL Image
            block_img_rgb = cv2.cvtColor(block_img, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(block_img_rgb)

            try:
                text, token_ids = self.vlm_recognizer(
                    image=pil_image, max_length=max_length)
            except Exception as e:
                logger.error(f"  Error processing block: {e}")
                text = ''

            # ä½¿ç”¨ markdown_converter è¿›è¡Œåå¤„ç†
            if 'table' in block_label:
                text = markdown_converter._handle_table(text)
            elif 'formula' in block_label and block_label != 'formula_number':
                text = markdown_converter._handle_formula(text)
            else:
                text = markdown_converter._handle_text(text)

            vl_rec_results.append(text)

        # ç»„è£…
        recognition_results = []
        curr_vlm_block_idx = 0

        for j, block in enumerate(blocks):
            block_img = block['img']
            block_bbox = block['box']
            block_label = block['label']
            block_content = ''

            if curr_vlm_block_idx < len(
                    vlm_block_ids) and vlm_block_ids[curr_vlm_block_idx] == j:
                result_str = vl_rec_results[curr_vlm_block_idx]
                figure_token_map = figure_token_maps[curr_vlm_block_idx]
                curr_vlm_block_idx += 1

                if result_str is None:
                    result_str = ''

                # æˆªæ–­é‡å¤å†…å®¹
                result_str = truncate_repetitive_content(result_str)

                # å¤„ç†å…¬å¼ç¬¦å·æ›¿æ¢
                has_paren = '\\(' in result_str and '\\)' in result_str
                has_bracket = '\\[' in result_str and '\\]' in result_str
                if has_paren or has_bracket:
                    result_str = result_str.replace('$', '')
                    result_str = (result_str.replace('\\(', ' $ ').replace(
                        '\\)', ' $ ').replace('\\[',
                                              ' $$ ').replace('\\]', ' $$ '))
                    if block_label == 'formula_number':
                        result_str = result_str.replace('$', '')

                # å¯¹ table ç»“æœè¿›è¡Œ OTSL è½¬ HTML å’Œ untokenize
                if 'table' in block_label:
                    html_str = convert_otsl_to_html(result_str)
                    if html_str != '':
                        result_str = html_str
                    result_str = untokenize_figure_of_table(
                        result_str, figure_token_map)

                block_content = result_str

            # å¤„ç†å›¾åƒç±»æ ‡ç­¾ï¼ˆå»é™¤ç¼–å·åç¼€åˆ¤æ–­ï¼‰
            base_label = block_label.rsplit(
                '_', 1)[0] if '_' in block_label and block_label.rsplit(
                    '_', 1)[1].isdigit() else block_label

            # åˆ¤æ–­æ˜¯å¦æ˜¯åˆå¹¶å—çš„åç»­éƒ¨åˆ†ï¼ˆimg ä¸º None è¡¨ç¤ºæ˜¯åˆå¹¶å—çš„åç»­éƒ¨åˆ†ï¼‰
            is_merged_continuation = block_img is None

            if base_label in image_labels and block_img is not None:
                x_min, y_min, x_max, y_max = list(map(int, block_bbox))
                img_path = f'imgs/img_in_{base_label}_box_{x_min}_{y_min}_{x_max}_{y_max}.jpg'
                # ä¸è·³è¿‡è¡¨æ ¼ä¸­çš„å›¾ç‰‡ï¼Œéœ€è¦ä¿å­˜å®ƒä»¬
                # if img_path in drop_figures_set:
                #     continue
                recognition_results.append({
                    'label': block_label,
                    'bbox': block_bbox,
                    'score': block.get('score', 1.0),
                    'text': '',
                    'text_unirec': '',
                    'is_image': True,
                    'img_path': img_path,
                    'is_merged_continuation': False,
                    'in_table': img_path in drop_figures_set  # æ ‡è®°æ˜¯å¦åœ¨è¡¨æ ¼ä¸­
                })
            else:
                recognition_results.append({
                    'label': block_label,
                    'bbox': block_bbox,
                    'score': block.get('score', 1.0),
                    'text': block_content,
                    'text_unirec': block_content,
                    'is_image': False,
                    'is_merged_continuation': is_merged_continuation
                })

        total_time = time.time() - start_time
        logger.info(f"  Total time: {total_time: .3f}s")

        result = {
            'input_path': actual_path if not is_temp_file else '<numpy_array>',
            'width': ori_w,
            'height': ori_h,
            'layout_results': layout_results,
            'recognition_results': recognition_results,
            'blocks': blocks,
            'timing': {
                'total': total_time,
            }
        }

        # Clean up temporary file if created
        if is_temp_file and os.path.exists(actual_path):
            os.remove(actual_path)

        return result

    def save_to_json(self, result: Dict, output_path: str):
        """ä¿å­˜ç»“æœä¸ºJSON"""
        if 'layout_results' in result:
            del result['layout_results']

        if 'blocks' in result:
            del result['blocks']

        img_name, img_dir = _get_image_name_and_dir(result, output_path)
        json_path = os.path.join(img_dir, f"{img_name}.json")

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        # logger.info(f"  Saved JSON to {json_path}")

    def save_to_markdown(self, result: Dict, output_path: str):
        """ä¿å­˜ç»“æœä¸ºMarkdownï¼ŒæŒ‰é˜…è¯»é¡ºåºåŒ…å«å›¾ç‰‡"""
        img_name, img_dir = _get_image_name_and_dir(result, output_path)
        md_path = os.path.join(img_dir, f"{img_name}.md")

        # åˆ›å»ºimgså­ç›®å½•
        imgs_dir = os.path.join(img_dir, 'imgs')
        os.makedirs(imgs_dir, exist_ok=True)

        # è¯»å–åŸå§‹å›¾åƒç”¨äºè£å‰ªä¿å­˜å›¾ç‰‡
        original_image = cv2.imread(result['input_path'])
        ori_width = result.get(
            'width',
            original_image.shape[1] if original_image is not None else 1)

        # ä¿å­˜æ‰€æœ‰å›¾ç‰‡åŒºåŸŸï¼ˆåŒ…æ‹¬è¡¨æ ¼ä¸­çš„å›¾ç‰‡ï¼‰
        if original_image is not None:
            for rec in result['recognition_results']:
                if rec.get('is_image', False):
                    img_path = rec.get('img_path', '')
                    if img_path:
                        bbox = rec.get('bbox', [])
                        if bbox:
                            x1, y1, x2, y2 = map(int, bbox)
                            cropped_img = original_image[y1:y2, x1:x2]
                            if cropped_img.size > 0:
                                save_img_path = os.path.join(img_dir, img_path)
                                os.makedirs(os.path.dirname(save_img_path), exist_ok=True)
                                cv2.imwrite(save_img_path, cropped_img)

        with open(md_path, 'w', encoding='utf-8') as f:
            pending_text = []  # ç”¨äºæ”¶é›†åˆå¹¶å—çš„æ–‡æœ¬
            pending_label = None  # å½“å‰åˆå¹¶å—çš„æ ‡ç­¾ç±»å‹

            for rec in result['recognition_results']:
                # è·å–åŸºç¡€æ ‡ç­¾åï¼ˆå»é™¤ç¼–å·åç¼€ï¼Œå¦‚ text_01 -> textï¼‰
                label = rec['label']
                base_label = label.rsplit(
                    '_', 1)[0] if '_' in label and label.rsplit(
                        '_', 1)[1].isdigit() else label

                # è·³è¿‡å¿½ç•¥çš„æ ‡ç­¾
                if base_label in self.markdown_ignore_labels:
                    continue

                # å¤„ç†å›¾ç‰‡ç±»å‹
                if rec.get('is_image', False):
                    # å…ˆè¾“å‡ºä¹‹å‰ç´¯ç§¯çš„æ–‡æœ¬
                    if pending_text:
                        self._write_merged_text(f, pending_text, pending_label)
                        pending_text = []
                        pending_label = None

                    # å¦‚æœå›¾ç‰‡åœ¨è¡¨æ ¼ä¸­ï¼Œè·³è¿‡åœ¨markdownä¸­ç‹¬ç«‹æ˜¾ç¤ºï¼ˆå·²åœ¨è¡¨æ ¼HTMLä¸­å¼•ç”¨ï¼‰
                    if rec.get('in_table', False):
                        continue

                    img_path = rec.get('img_path', '')
                    if img_path:
                        # è®¡ç®—å›¾ç‰‡å®½åº¦å åŸå›¾çš„ç™¾åˆ†æ¯”
                        bbox = rec.get('bbox', [])
                        if bbox:
                            x1, y1, x2, y2 = map(int, bbox)
                            img_width = x2 - x1
                            width_percent = int((img_width / ori_width) * 100)
                            width_percent = max(5, min(width_percent, 100))  # é™åˆ¶åœ¨5%-100%ä¹‹é—´
                        else:
                            width_percent = 50  # é»˜è®¤50%
                        f.write(
                            f'<img src="{img_path}" alt="Image" width="{width_percent}%" />\\n\\n'
                        )
                    continue
                text = rec['text'].strip()
                if not text:
                    continue

                # æ£€æŸ¥æ˜¯å¦æ˜¯åˆå¹¶å—çš„åç»­éƒ¨åˆ†
                is_merged_continuation = rec.get('is_merged_continuation', False)

                if is_merged_continuation and pending_text:
                    # æ˜¯åˆå¹¶å—çš„åç»­éƒ¨åˆ†ï¼Œè¿½åŠ æ–‡æœ¬
                    pending_text.append(text)
                else:
                    # å…ˆè¾“å‡ºä¹‹å‰ç´¯ç§¯çš„æ–‡æœ¬
                    if pending_text:
                        self._write_merged_text(f, pending_text, pending_label)
                        pending_text = []
                        pending_label = None

                    # å¼€å§‹æ–°çš„æ–‡æœ¬å—
                    pending_text.append(text)
                    pending_label = base_label

            # è¾“å‡ºæœ€åç´¯ç§¯çš„æ–‡æœ¬
            if pending_text:
                self._write_merged_text(f, pending_text, pending_label)

    def _write_merged_text(self, f, texts: List[str], base_label: str):
        """å°†åˆå¹¶çš„æ–‡æœ¬å†™å…¥æ–‡ä»¶"""
        merged_text = ' '.join(texts)

        # æ ¹æ®æ ‡ç­¾ç±»å‹æ ¼å¼åŒ–è¾“å‡º
        if 'title' in base_label or base_label == 'doc_title':
            f.write(f"## {merged_text}\n\n")
        elif 'table' in base_label:
            f.write(f"{merged_text}\n\n")
        elif 'formula' in base_label or base_label == 'equation':
            f.write(f"$${merged_text}$$\n\n")
        else:
            f.write(f"{merged_text}\n\n")

    def save_visualization(self, result: Dict, output_path: str):
        """ä¿å­˜å¯è§†åŒ–ç»“æœ"""
        img_name, img_dir = _get_image_name_and_dir(result, output_path)
        vis_path = os.path.join(img_dir, f"{img_name}_vis.jpg")

        image = cv2.imread(result['input_path'])

        for box_info in result['layout_results']['boxes']:
            x1, y1, x2, y2 = map(int, box_info['coordinate'])
            label = box_info['label']
            score = box_info['score']

            # æå–åŸºç¡€æ ‡ç­¾åï¼ˆå»é™¤ç¼–å·åç¼€ï¼Œå¦‚ text_01 -> textï¼‰
            base_label = label.rsplit('_', 1)[0] if '_' in label and label.rsplit('_', 1)[1].isdigit() else label

            # è·å–é¢œè‰²ï¼Œå¦‚æœæ²¡æœ‰å®šä¹‰åˆ™ä½¿ç”¨é»˜è®¤çº¢è‰²
            color = self.colors.get(base_label, (255, 0, 0))

            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
            cv2.putText(image, f"{label}: {score: .2f}", (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        cv2.imwrite(vis_path, image)
        # logger.info(f"  Saved visualization to {vis_path}")


# ==================== Main Function ====================
def main():
    desc = 'OpenDoc ONNX Pipeline - Full Document OCR with Layout Detection'
    parser = argparse.ArgumentParser(description=desc)

    # Input/Output
    parser.add_argument('--input_path',
                        type=str,
                        required=True,
                        help='Path to input image or directory')
    parser.add_argument('--output_path',
                        type=str,
                        default='./output_onnx',
                        help='Path to save output results')

    # Model paths
    parser.add_argument('--layout_model',
                        type=str,
                        default=None,
                        help='Path to layout detection ONNX model (default: ~/.cache/openocr/PP_DoclayoutV2_onnx/PP-DoclayoutV2.onnx)')
    parser.add_argument('--encoder_model',
                        type=str,
                        default=None,
                        help='Path to UniRec encoder ONNX model (default: ~/.cache/openocr/unirec_0_1b_onnx/unirec_encoder.onnx)')
    parser.add_argument('--decoder_model',
                        type=str,
                        default=None,
                        help='Path to UniRec decoder ONNX model (default: ~/.cache/openocr/unirec_0_1b_onnx/unirec_decoder.onnx)')
    parser.add_argument('--tokenizer_mapping',
                        type=str,
                        default=None,
                        help='Path to tokenizer mapping JSON file (default: ~/.cache/openocr/unirec_0_1b_onnx/unirec_tokenizer_mapping.json)')

    # Settings
    parser.add_argument('--use-gpu',
                        type=str,
                        default='auto',
                        choices=['auto', 'true', 'false'],
                        help='Use GPU for inference (auto: auto-detect, true: force GPU, false: force CPU)')
    parser.add_argument('--layout_threshold',
                        type=float,
                        default=0.4,
                        help='Layout detection threshold')
    parser.add_argument('--max_length',
                        type=int,
                        default=2048,
                        help='Maximum generation length for VLM')
    parser.add_argument('--use_layout_detection',
                        action='store_true',
                        help='Use layout detection')
    parser.add_argument('--no_layout_detection',
                        dest='use_layout_detection',
                        action='store_false',
                        help='Disable layout detection (process whole image)')
    parser.add_argument('--use_chart_recognition',
                        action='store_true',
                        help='Recognize charts')
    parser.add_argument('--no-auto-download',
                        action='store_true',
                        help='Disable automatic model download')

    # Output formats
    parser.add_argument('--save_vis',
                        action='store_true',
                        help='Save visualization images')
    parser.add_argument('--save_json',
                        action='store_true',
                        help='Save JSON results')
    parser.add_argument('--save_markdown',
                        action='store_true',
                        help='Save Markdown results')

    args = parser.parse_args()

    # Parse use_gpu argument
    if args.use_gpu == 'auto':
        use_gpu = None
    elif args.use_gpu == 'true':
        use_gpu = True
    else:
        use_gpu = False

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_path, exist_ok=True)

    opendoc_onnx = OpenDocONNX(
        layout_model_path=args.layout_model,
        unirec_encoder_path=args.encoder_model,
        unirec_decoder_path=args.decoder_model,
        tokenizer_mapping_path=args.tokenizer_mapping,
        use_gpu=use_gpu,
        layout_threshold=args.layout_threshold,
        use_layout_detection=args.use_layout_detection,
        use_chart_recognition=args.use_chart_recognition,
        auto_download=not args.no_auto_download,
    )

    # è·å–å›¾åƒåˆ—è¡¨
    img_list = get_image_file_list(args.input_path)
    logger.info(f'\nFound {len(img_list)} images in {args.input_path}')
    logger.info(f'Output will be saved to: {args.output_path}')
    logger.info('=' * 80)

    # å¤„ç†æ¯å¼ å›¾åƒ
    for idx, img_path in enumerate(img_list):
        logger.info(
            f"\n[{idx + 1}/{len(img_list)}] Processing: {os.path.basename(img_path)}"
        )

        try:
            # é¢„æµ‹
            result = opendoc_onnx(
                img_path=img_path,
                layout_threshold=args.layout_threshold,
                max_length=args.max_length,
            )

            # ä¿å­˜ç»“æœ
            if args.save_vis:
                opendoc_onnx.save_visualization(result, args.output_path)

            if args.save_json:
                opendoc_onnx.save_to_json(result, args.output_path)

            if args.save_markdown:
                opendoc_onnx.save_to_markdown(result, args.output_path)

        except Exception as e:
            logger.error(f"Error processing {img_path}: {str(e)}")
            import traceback
            traceback.print_exc()
            continue

    logger.info('\n' + '=' * 80)
    logger.info(
        f'âœ… All processing completed! Results saved to {args.output_path}')
    logger.info('=' * 80)


if __name__ == '__main__':
    main()
